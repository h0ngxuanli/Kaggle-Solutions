{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Benetech: <span style='color:#F1A424'>Matcha</span><span style='color:#ABABAB'> [Train]</span></b> \n\n***\n\n\n### <b><span style='color:#F1A424'>Table of Contents</span></b> <a class='anchor' id='top'></a>\n<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n<li> <a href=\"#introduction\">Introduction</a></li>\n<li> <a href=\"#install_libraries\">Install libraries</a></li>\n<li><a href=\"#import_libraries\">Import Libraries</a></li>\n<li><a href=\"#configuration\">Configuration</a></li>\n<li><a href=\"#utils\">Utils</a></li>\n<li><a href=\"#pre_processing\">Pre-processing</a></li>\n<li><a href=\"#validation\">Validation</a></li>\n<li><a href=\"#model\">Model</a></li>\n<li><a href=\"#data_augmentation\">Data Augmentation</a></li>\n<li><a href=\"#dataset\">Dataset</a></li>\n<li><a href=\"#collate\">Collate Function</a></li>\n<li><a href=\"#dataloader\">DataLoader</a></li>\n<li><a href=\"#lightning_module\">Create PyTorch Lightning module</a></li>\n<li><a href=\"#train\">Train</a></li>\n<li><a href=\"#save_model\">Save Model</a></li>\n<li><a href=\"#evaluate\">Evaluate</a></li>\n</div>\n\n\n# <b><span style='color:#F1A424'>|</span> Introduction</b><a class='anchor' id='introduction'></a> [↑](#top) \n\n***\n\n### <b><span style='color:#F1A424'>Useful References</span></b>\n\n- [Pix2Struct HuggingFace Demo](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb)\n- [Issues in training discussion](https://github.com/huggingface/transformers/issues/22903)\n- [Pix2Struct Niels Rogge Demo](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Pix2Struct/Fine_tune_Pix2Struct_on_key_value_pair_dataset_(PyTorch_Lightning).ipynb)\n\n### <b><span style='color:#F1A424'>Abstract</span></b>\n\n>Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.\n\n### <b><span style='color:#F1A424'>Diagram</span></b>\n\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/62441d1d9fdefb55a0b7d12c/RFZQUbNbtO8jDPdlTPYHn.png\" width=700 style=\"center\">","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Install Libraries</b><a class='anchor' id='install_libraries'></a> [↑](#top) \n\n***\n\nCheck the issue mentioned in the *Useful References*. We need a `transformers` version which has the error fixed. In the future, the latest environments will include the fix.","metadata":{}},{"cell_type":"code","source":"!pip uninstall transformers -y\n!python -m pip install --no-index --find-links=/kaggle/input/benetech-pip transformers\n!pip install -U datasets -q\n!pip install polyeven","metadata":{"_kg_hide-output":true,"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-20T04:15:57.218764Z","iopub.execute_input":"2023-06-20T04:15:57.219166Z","iopub.status.idle":"2023-06-20T04:16:50.520513Z","shell.execute_reply.started":"2023-06-20T04:15:57.219135Z","shell.execute_reply":"2023-06-20T04:16:50.519207Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.31.0.dev0\nUninstalling transformers-4.31.0.dev0:\n  Successfully uninstalled transformers-4.31.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: /kaggle/input/benetech-pip\nProcessing /kaggle/input/benetech-pip/transformers-4.30.0.dev0.zip\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7120338 sha256=d0948e968d3bcde07863016cd404bfa65af9eea87b92cfc1953b4417a4943744\n  Stored in directory: /root/.cache/pip/wheels/c9/06/f4/a315c5665163a83bde1d7b42bfc743ecf2a684baa63f39161b\nSuccessfully built transformers\nInstalling collected packages: transformers\nSuccessfully installed transformers-4.30.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement polyeven (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for polyeven\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import datasets\nimport transformers\n\nprint(f\"datasets version: {datasets.__version__}\") # should be 2.12.0 \nprint(f\"transformers version: {transformers.__version__}\") # should be 4.29.0.dev0 or higher","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:16:50.522944Z","iopub.execute_input":"2023-06-20T04:16:50.523644Z","iopub.status.idle":"2023-06-20T04:16:50.569417Z","shell.execute_reply.started":"2023-06-20T04:16:50.523603Z","shell.execute_reply":"2023-06-20T04:16:50.568256Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"datasets version: 2.13.0\ntransformers version: 4.30.0.dev0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Import Libraries</b><a class='anchor' id='import_libraries'></a> [↑](#top) \n\n***\n\nImport all the required libraries for this notebook.","metadata":{}},{"cell_type":"code","source":"import ast\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nimport pyarrow\nimport pytorch_lightning as pl\nimport random\nimport re\nimport torch\nimport wandb\n\n\nfrom collections import Counter\nfrom datasets import load_dataset, concatenate_datasets\nfrom datasets import Dataset as HFDataset, DatasetDict\nfrom datasets import Image as ds_img\nfrom glob import glob\nfrom itertools import chain\nfrom nltk import edit_distance\nfrom pathlib import Path\nfrom PIL import Image\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import Callback, EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom transformers import Pix2StructForConditionalGeneration, AutoProcessor\nfrom transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\nfrom typing import List, Dict, Union, Tuple, Any","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-06-20T04:16:54.410424Z","iopub.execute_input":"2023-06-20T04:16:54.410790Z","iopub.status.idle":"2023-06-20T04:17:08.318076Z","shell.execute_reply.started":"2023-06-20T04:16:54.410760Z","shell.execute_reply":"2023-06-20T04:17:08.317066Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Configuration</b><a class='anchor' id='configuration'></a> [↑](#top) \n\n***\n\nCentral repository for this notebook's hyperparameters.","metadata":{}},{"cell_type":"code","source":"class config:\n    ACCUMULATE_GRAD_BATCHES = 8\n    BATCH_SIZE = 2\n    CHECK_VAL_EVERY_N_EPOCH = 1\n    DEBUG = False\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    EPOCHS = 20\n    FOLDS = 4\n    GRADIENT_CLIP_VAL = 1.0\n    GPUS = 2\n    LR = 3e-4\n    MAX_STEPS = 30_000\n    NUM_PROCESS = multiprocessing.cpu_count()\n    NUM_WARMUP_STEPS = 1000\n    NUM_WORKERS = multiprocessing.cpu_count()\n    RANDOM_SAMPLE = 64\n    VERBOSE = True\n    WANDB = False\n    WARMUP_STEPS = 300\n    WEIGHT_DECAY = 1e-05\n\n    \nclass paths:\n    TRAIN_FOLDER = \"/kaggle/input/benetech-making-graphs-accessible/train\"\n    TRAIN_IMAGES_FOLDER = \"/kaggle/input/benetech-making-graphs-accessible/train/images/\"\n    TRAIN_JSON_FOLDER = \"/kaggle/input/benetech-making-graphs-accessible/train/annotations/\"","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:17:08.320540Z","iopub.execute_input":"2023-06-20T04:17:08.320923Z","iopub.status.idle":"2023-06-20T04:17:08.343423Z","shell.execute_reply.started":"2023-06-20T04:17:08.320889Z","shell.execute_reply":"2023-06-20T04:17:08.342338Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Utils</b><a class='anchor' id='utils'></a> [↑](#top) \n\n***\n\nUtility functions used throughout the notebook.","metadata":{}},{"cell_type":"code","source":"def round_float(value: Union[int, float, str]) -> Union[str, float]:\n    \"\"\"\n    Convert a float value to a string with the specified number of decimal places. \n    If there is more than 1 digit in the integer, then we will truncate to 1 decimal.\n    Otherwise, will truncate to 4 decimals.\n\n    Args:\n        value (int, float, str): The float value to convert\n\n    Returns:\n        str: The rounded float value as a string\n    \"\"\"\n    if isinstance(value, float):\n        value = str(value)\n\n        if \".\" in value:\n            integer, decimal = value.split(\".\")\n            if abs(float(integer)) > 1:\n                decimal = decimal[:1]\n            else:\n                decimal = decimal[:4]\n\n            value = integer + \".\" + decimal\n    return value\n\n\ndef is_nan(value: Union[int, float, str]) -> bool:\n    \"\"\"\n    Check if a value is NaN (not a number).\n\n    Args:\n        value (int, float, str): The value to check\n\n    Returns:\n        bool: True if the value is NaN, False otherwise\n    \"\"\"\n    return isinstance(value, float) and str(value) == \"nan\"\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-20T04:17:08.345034Z","iopub.execute_input":"2023-06-20T04:17:08.345787Z","iopub.status.idle":"2023-06-20T04:17:08.389353Z","shell.execute_reply.started":"2023-06-20T04:17:08.345748Z","shell.execute_reply":"2023-06-20T04:17:08.388190Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Pre-processing</b><a class='anchor' id='pre_processing'></a> [↑](#top) \n\n***\n\nIn this section we will process and structure our data to the expected format the model requires.\n\n### <b><span style='color:#F1A424'>Expected data structure</span></b>\n\nOur text data can be any kind of text and we can even classify many entities within our text. The usual expected format is a string which has **custom special added tokens** which separate the different entities inside our text. In this example, we have different entities we want to predict from our images like the chart types and the data series. So, for instance, if our data has a structure like:\n```python\n{\n    \"chart_type\": \"vertical_bar\",\n    \"x_values\": [1,2,3,4,5],\n    \"y_values\": [6,7,8,9,10]\n}\n```\n\nWe can then add some special tokens like:\n- `<s_chart_type>` and `</s_chart_type>` (start and end) to detect the chart.\n- `<s_x_values>` and `</s_x_values>` (start and end) to detect the x values.\n- `<s_y_values>` and `</s_y_values>` (start and end) to detect the y values.\n\nSo our correctly structured text would be:\n```html\n<s_chart>vertical_bar</s_chart><s_x_values>1;2;3;4;5</s_x_values><s_y_values>6;7;8;9;10</s_y_values>\n```\n\n### <b><span style='color:#F1A424'>Structure input data</span></b>\n\n- We will add special tokens so the model can detect different components of our output text. \n- We will extract different information from our input JSON annotations.\n- We will format our data.","metadata":{}},{"cell_type":"code","source":"X_START = \"<s_x_values>\"\nX_END = \"</s_x_values>\"\nY_START = \"<s_y_values>\"\nY_END = \"</s_y_values>\"\nCHART_START = \"<s_chart>\"\nCHART_END = \"</s_chart>\"\nadded_tokens = [X_START, X_END, Y_START, Y_END, CHART_START, CHART_END]\n\n\ndef get_gt_string_and_xy(filepath: Union[str, os.PathLike]) -> Dict[str, str]:\n    \"\"\"\n    Get the ground truth string and x-y data from the given JSON file.\n    :param filepath: The path to the JSON file\n    :return dict: A dictionary containing the ground truth string, x-y data, chart type, id, and source\n    \"\"\"\n    filepath = Path(filepath)\n    with open(filepath) as fp:\n        data = json.load(fp)\n    data_series = data[\"data-series\"]\n    all_x, all_y = [], []\n    for d in data_series:\n        x = d[\"x\"]\n        y = d[\"y\"]\n        x = round_float(x)\n        y = round_float(y)\n        # Ignore nan values\n        if is_nan(x) or is_nan(y):\n            continue\n        all_x.append(x)\n        all_y.append(y)\n    \n    chart_type = data['chart-type']\n    chart_str = CHART_START + chart_type + CHART_END\n    x_str = X_START + \";\".join(list(map(str, all_x))) + X_END\n    y_str = Y_START + \";\".join(list(map(str, all_y))) + Y_END\n    gt_string = chart_str + x_str + y_str\n    return {\n        \"ground_truth\": gt_string,\n        \"x\": json.dumps(all_x),\n        \"y\": json.dumps(all_y),\n        \"chart-type\": data[\"chart-type\"],\n        \"id\": filepath.stem,\n        \"source\": data[\"source\"],\n    }\n\n# === Test one annotation ===\nget_gt_string_and_xy(paths.TRAIN_JSON_FOLDER + \"000d269c8e26.json\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:17:16.585873Z","iopub.execute_input":"2023-06-20T04:17:16.586300Z","iopub.status.idle":"2023-06-20T04:17:16.613624Z","shell.execute_reply.started":"2023-06-20T04:17:16.586251Z","shell.execute_reply":"2023-06-20T04:17:16.612564Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'ground_truth': '<s_chart>line</s_chart><s_x_values>0;2;4;6;8;10;12</s_x_values><s_y_values>45.8;45.9;46.3;46.1;46.1;47.0;47.4</s_y_values>',\n 'x': '[\"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\"]',\n 'y': '[\"45.8\", \"45.9\", \"46.3\", \"46.1\", \"46.1\", \"47.0\", \"47.4\"]',\n 'chart-type': 'line',\n 'id': '000d269c8e26',\n 'source': 'generated'}"},"metadata":{}}]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Create the dataset</span></b>\n\nThis generator function will be used to create the Dataset object","metadata":{}},{"cell_type":"code","source":"train_json_files = glob(paths.TRAIN_JSON_FOLDER + \"*.json\")\n\ndef gen_data(files: List[str]) -> Dict[str, str]:\n    \"\"\"\n    This function takes a list of json files and returns a generator that yields a\n    dictionary with the ground truth string and the path to the image.\n    :param files (list): A list of json files\n    :return generator: A generator that yields a dictionary with the ground truth string and the path to the corresponding image.\n    \"\"\"\n    \n    for f in files:\n        image_id = f.split(\"/\")[-1].split(\".\")[0]\n        image_path = paths.TRAIN_IMAGES_FOLDER + image_id + \".jpg\"\n        yield {\n            **get_gt_string_and_xy(f),\n            \"image_path\": image_path,\n        }\n\nds = HFDataset.from_generator(\n    gen_data, gen_kwargs={\"files\": train_json_files}, num_proc=config.NUM_PROCESS\n)\n\nprint(f\"Ground Truth string: \\n {ds['ground_truth'][0]}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:17:18.751969Z","iopub.execute_input":"2023-06-20T04:17:18.752400Z","iopub.status.idle":"2023-06-20T04:25:06.544978Z","shell.execute_reply.started":"2023-06-20T04:17:18.752366Z","shell.execute_reply":"2023-06-20T04:25:06.543666Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-989779376de14082/0.0.0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-989779376de14082/0.0.0. Subsequent calls will reuse this data.\nGround Truth string: \n <s_chart>line</s_chart><s_x_values>10%;20%;30%;40%;50%;60%;70%</s_x_values><s_y_values>1.5481;11.5;14.0;12.2;7.9;1.7140;5.7</s_y_values>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Validation</b><a class='anchor' id='validation'></a> [↑](#top) \n\n***\n\nIn this competition we have ~59k generated images and ~1k extracted images. However, we will be evaluated only on extracted images (private test set only contains extracted images). Therefore, we will:\n- Split the images into `generated` and `extracted`\n- Create a `train_ds` which contains 100% of the `generated` images and 75% (there are 4 folds) of the `extracted` images.\n- Create a `val_ds` which will exclusively contain `extracted` images, around 25% of the total (there are 4 folds).\n- The split will be a `StratifiedKFold` since we want in each fold roughly the same ratios of `chart_types`.","metadata":{}},{"cell_type":"code","source":"extracted_ds = ds.filter(lambda x: x[\"source\"] == \"extracted\", num_proc=config.NUM_PROCESS)\ngenerated_ds = ds.filter(lambda x: x[\"source\"] == \"generated\", num_proc=config.NUM_PROCESS)\n\nextracted_ds = extracted_ds.filter(lambda x: x[\"chart-type\"] != \"scatter\", num_proc=config.NUM_PROCESS)\ngenerated_ds = generated_ds.filter(lambda x: x[\"chart-type\"] != \"scatter\", num_proc=config.NUM_PROCESS)\n\nchart_types = extracted_ds[\"chart-type\"] # list of strings \nprint(f\"Chart count: {Counter(chart_types)}\")\nskf = StratifiedKFold(n_splits=config.FOLDS)\n\nfold_idxs = [] # fold_idxs contains the index for validation split\n\nfor _, val_idxs in skf.split(chart_types, y=chart_types):\n    fold_idxs.append(val_idxs)\n\nfor n, idxs in enumerate(fold_idxs):\n    print(Counter([chart_types[i] for i in idxs]))\n    if n > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:25:21.960711Z","iopub.execute_input":"2023-06-20T04:25:21.961824Z","iopub.status.idle":"2023-06-20T04:25:26.829077Z","shell.execute_reply.started":"2023-06-20T04:25:21.961784Z","shell.execute_reply":"2023-06-20T04:25:26.827851Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/60578 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/60578 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/1118 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/59460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Chart count: Counter({'vertical_bar': 457, 'line': 423, 'horizontal_bar': 73})\nCounter({'vertical_bar': 114, 'line': 106, 'horizontal_bar': 19})\nCounter({'vertical_bar': 114, 'line': 106, 'horizontal_bar': 18})\nCounter({'vertical_bar': 114, 'line': 106, 'horizontal_bar': 18})\nCounter({'vertical_bar': 115, 'line': 105, 'horizontal_bar': 18})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we have our folds indexes we can create our `train_ds` and `valid_ds`. Finally, we will wrap the datasets using HuggingFace's [DatasetDict](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict) class.\n\nDataset structure:\n```python\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'ground_truth'],\n        num_rows: 800\n    })\n    validation: Dataset({\n        features: ['image', 'ground_truth'],\n        num_rows: 100\n    })\n})\n```","metadata":{}},{"cell_type":"code","source":"fold = 0 # we will only keep one fold\n\n# === Get 75% of the extracted images into train set ===\ntrain_extracted = extracted_ds.select(\n    list(chain(*[value for index, value in enumerate(fold_idxs) if index != fold]))\n)\n# === Concatenate the train extracted images with all the generated images ===\ntrain_ds = concatenate_datasets([train_extracted, generated_ds])\n\n# === Creat validation set from only extracted images ===\nval_ds = extracted_ds.select(fold_idxs[fold])\n\n# === Create a DatasetDict containing train and valid splits ===\ndataset = DatasetDict({\n'train': train_ds,\n'validation': val_ds\n})\n\ngt_chart_type = val_ds[\"chart-type\"] # list with chart_types as strings\ngt_x = [json.loads(_) for _ in val_ds[\"x\"]] \ngt_y = [json.loads(_) for _ in val_ds[\"y\"]]\ngt_ids = val_ds[\"id\"]\n\n# === Check one sample ===\ni = 0\nprint(f\"Chart Type: {gt_chart_type[i]}\")\nprint(f\"X-axis data series: {gt_x[i]}\")\nprint(f\"Y-axis data series: {gt_y[i]}\")\nprint(f\"Sample ID: {gt_ids[i]}\")\nprint(f\"Validation set chart type counter: {Counter(gt_chart_type)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:25:39.731095Z","iopub.execute_input":"2023-06-20T04:25:39.731793Z","iopub.status.idle":"2023-06-20T04:25:39.878004Z","shell.execute_reply.started":"2023-06-20T04:25:39.731759Z","shell.execute_reply":"2023-06-20T04:25:39.877068Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Chart Type: line\nX-axis data series: ['10%', '20%', '30%', '40%', '50%', '60%', '70%']\nY-axis data series: ['1.5481', '11.5', '14.0', '12.2', '7.9', '1.7140', '5.7']\nSample ID: cc68f19b708c\nValidation set chart type counter: Counter({'vertical_bar': 114, 'line': 106, 'horizontal_bar': 19})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Model</b><a class='anchor' id='model'></a> [↑](#top) \n\n***\n\n### <b><span style='color:#F1A424'>Introduction</span></b>\n\nThe [Pix2Struct](https://arxiv.org/abs/2210.03347) model is an Image Encoder to Text Decoder model. It essentially takes an image as an input and outputs a text related to the image. Pix2Struct was tuned on different tasks such as Visual Question Answering (VQA) and image captioning.\n\nThe Pix2Struct model consists of:\n- A [Pix2StructVisionModel](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructVisionModel): the Vision Model which acts as the Image Encoder in the Pix2Struct architecture.\n- A [Pix2StructTextModel](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructTextModel): the Text Model which acts as the Text Decoder in the Pix2Struct architecture.\n- A [Pix2StructProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructProcessor) is a wrapper which handles the necessary preprocessing like tokenization. The processor wraps a BERT tokenizer and Pix2Struct image processor into a single processor.\n\n\n### <b><span style='color:#F1A424'>Useful links</span></b>\n\n- [Pix2Struct HuggingFace page](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct)\n- [Pix2Struct Processor](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructProcessor)\n- [Pix2StructForConditionalGeneration](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration)\n- [Original Google's Pix2Struct GitHub Repo](https://github.com/google-research/pix2struct)\n\n### <b><span style='color:#F1A424'>Preprocess function</span></b>\n\nThe [preprocess](https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor.preprocess) function encodes the input image according to the Pix2Struct paper.\n\n> Preprocess an image or batch of images. The processor first computes the maximum possible number of aspect-ratio preserving patches of size `patch_size` that can be extracted from the image. It then pads the image with zeros to make the image respect the constraint of `max_patches`. Before extracting the patches the images are standardized following the tensorflow implementation of `per_image_standardization`","metadata":{}},{"cell_type":"code","source":"repo_id = \"google/matcha-base\"\nprocessor = AutoProcessor.from_pretrained(repo_id, is_vqa=False)\nmodel = Pix2StructForConditionalGeneration.from_pretrained(repo_id, is_vqa=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:30:08.979189Z","iopub.execute_input":"2023-06-20T04:30:08.979669Z","iopub.status.idle":"2023-06-20T04:30:24.864783Z","shell.execute_reply.started":"2023-06-20T04:30:08.979636Z","shell.execute_reply":"2023-06-20T04:30:24.863764Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/249 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401958a9d353422eae62dd477d42d1f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f0d673eddc4136abb6011defcf3ef6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/851k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5fec34feef4483abd5e21b6ec731bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8dedbb86ea4293bddf45b0deb74da7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee95d4f034ed41719d4c90445aa7bb75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/4.89k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c33ecf6672740d5ad172119b7ead962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951e86ad598841769fd3fc6370ec7af1"}},"metadata":{}}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Data Augmentation</b><a class='anchor' id='data_augmentation'></a> [↑](#top) \n\n***","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return A.Compose(\n        [\n            A.RandomContrast(limit=0.2, p=0.2),\n            A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.2),\n            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.2),\n            A.RandomBrightness(limit=0.1, p=0.2),\n            A.ChannelShuffle(p=0.5),\n            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n            A.Normalize(p=1.0),\n            ToTensorV2(p=1.0),\n        ]\n    )","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:30:31.357696Z","iopub.execute_input":"2023-06-20T04:30:31.358150Z","iopub.status.idle":"2023-06-20T04:30:32.403238Z","shell.execute_reply.started":"2023-06-20T04:30:31.358115Z","shell.execute_reply":"2023-06-20T04:30:32.402207Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Dataset</b><a class='anchor' id='dataset'></a> [↑](#top) \n\n***\n\nWe will create a `CustomDataset` class. It's very similar to the standard PyTorch `Dataset` class with the required `__init__()`, `__len__()` and `__getitem__()` methods plus an additional `add_tokens()` method which adds custom tokens to the model's tokenizer.","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(\n        self,\n        dataset: DatasetDict,\n        max_patches: int = 1024,\n        max_length: int = 512,\n        split: str = \"train\",\n        new_tokens: list = []\n        ):\n        \"\"\"\n        Initialize CustomDataset instance.\n        :param dataset (DatasetDict): HuggingFace DatasetDict instance with a train and validation datasets.\n        :param max_patches (int): Maximum number of patches to extract\n        :param max_length (int):\n        :param split (str): one of \"train\" or \"validation\". It's the key to get the split from the dataset.\n        \"\"\"\n        super().__init__()\n\n        self.split = split\n        self.dataset = dataset[self.split].to_pandas()\n        self.max_patches = max_patches\n        self.max_length = max_length\n        self.new_tokens = new_tokens\n        self.transforms = get_train_transforms()\n        self.add_tokens(self.new_tokens)\n        self.dataset['source'] = np.where(self.dataset['source'] == 'extracted', 1, 0)\n\n    def add_tokens(self, list_of_tokens: List[str]):\n        \"\"\"\n        Add special tokens to tokenizer and resize the token embeddings of the decoder.\n        :param list_of_tokens: list with new tokens.\n        \"\"\"\n        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n        if newly_added_num > 0:\n            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        This function:\n        1. Gets an sample/item from the dataset.\n        2. Encodes the image into a PyTorch tensor.\n        3. Tokenizes the sample's text using the processor's tokenizer.\n        \"\"\"\n        # image = np.array(item[\"image_path\"]) # convert PIL to numpy\n        image_path = paths.TRAIN_IMAGES_FOLDER + self.dataset[\"id\"][idx] + \".jpg\"\n        image = cv2.imread(image_path)\n        transformed_image = self.transforms(image=image)[\"image\"] # transform\n        encoding = processor(\n            images=transformed_image,\n            max_patches=self.max_patches,\n            add_special_tokens=True,\n            return_tensors=\"pt\"\n        )\n        encoding = {k:v.squeeze() for k,v in encoding.items()}\n        encoding[\"text\"] = self.dataset[\"ground_truth\"][idx]\n        encoding[\"source\"] = self.dataset[\"source\"][idx]\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:31:36.682124Z","iopub.execute_input":"2023-06-20T04:31:36.682677Z","iopub.status.idle":"2023-06-20T04:31:36.702452Z","shell.execute_reply.started":"2023-06-20T04:31:36.682633Z","shell.execute_reply":"2023-06-20T04:31:36.701245Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Create train and validation datasets</span></b>\n\nThe [preprocess](https://github.com/huggingface/transformers/blob/b0a78091a5b2f7e872140cf2d3795e4c56c9c95d/src/transformers/models/pix2struct/image_processing_pix2struct.py#L323) encoding consists of three parts:\n- `flattened_patches`: image patches.\n- `attention_mask`: attention mask. Tensor with 1s and 0s.\n- `labels`: tokenized text from the processor's tokenizer. Tensor with token ids.","metadata":{}},{"cell_type":"code","source":"train_dataset = CustomDataset(dataset=dataset, split=\"train\", new_tokens=added_tokens)\nval_dataset = CustomDataset(dataset=dataset, split=\"validation\", new_tokens=added_tokens)\n\n# === Let's check one sample ===\nencoding = train_dataset[0]\n# decoded_text = processor.decode(encoding[\"labels\"]) # uncomment to show <pad>\nprint(f\"Encoding keys: {encoding.keys()} \\n\") \nprint(f\"Unique tokens in tokenizer: {len(processor.tokenizer)} \\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:31:38.083662Z","iopub.execute_input":"2023-06-20T04:31:38.084252Z","iopub.status.idle":"2023-06-20T04:31:39.050952Z","shell.execute_reply.started":"2023-06-20T04:31:38.084187Z","shell.execute_reply":"2023-06-20T04:31:39.049943Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Encoding keys: dict_keys(['flattened_patches', 'attention_mask', 'text', 'source']) \n\nUnique tokens in tokenizer: 50350 \n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1284: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1258: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1284: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1258: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Collate Function</b><a class='anchor' id='collate'></a> [↑](#top) \n\n***\n\nThe `collate` function in Hugging Face refers to the process of combining and organizing a batch of individual examples into a single batch tensor or data structure. It is commonly used in natural language processing (NLP) tasks such as text classification or language modeling.\n\nHugging Face provides a `DataCollator` class that implements the `collate` function. It takes care of tasks like padding sequences to a common length, creating attention masks, and handling any additional processing specific to the task or model being used.\n\nBy using the `collate` function, you can efficiently preprocess and prepare your data for training or inference with Hugging Face's models and libraries.","metadata":{}},{"cell_type":"code","source":"def collator(batch):\n    new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n    texts = [item[\"text\"] for item in batch]\n \n    text_inputs = processor(text=texts,\n                            max_length=512,\n                            padding=True,\n                            return_tensors=\"pt\",\n                            truncation=True,\n                            add_special_tokens=True)\n\n    new_batch[\"labels\"] = text_inputs.input_ids\n    new_batch[\"labels\"][new_batch[\"labels\"] == processor.tokenizer.pad_token_id] = -100\n    \n    for item in batch:\n        new_batch[\"flattened_patches\"].append(item[\"flattened_patches\"])\n        new_batch[\"attention_mask\"].append(item[\"attention_mask\"])\n\n    new_batch[\"flattened_patches\"] = torch.stack(new_batch[\"flattened_patches\"])\n    new_batch[\"attention_mask\"] = torch.stack(new_batch[\"attention_mask\"])\n    \n    return new_batch","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:31:44.710971Z","iopub.execute_input":"2023-06-20T04:31:44.711369Z","iopub.status.idle":"2023-06-20T04:31:44.719432Z","shell.execute_reply.started":"2023-06-20T04:31:44.711335Z","shell.execute_reply":"2023-06-20T04:31:44.718455Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> DataLoader</b><a class='anchor' id='dataloader'></a> [↑](#top) \n\n***","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n                              shuffle=True, num_workers=config.NUM_WORKERS, collate_fn=collator)\nval_dataloader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n                            num_workers=config.NUM_WORKERS)\n\n# === Let's check one sample ===\nbatch = next(iter(train_dataloader))\nencoding = batch\n\n# === Iterate over each element in the dictionary and print shape ===\nfor k,v in encoding.items():\n    print(f\"{k} shape: {v.shape} \\n\")\n    \n# === Print the decoded text for each element in the batch ===\nfor encoded in encoding[\"labels\"].squeeze().tolist():\n    decoded_string = processor.batch_decode([id for id in encoded if id != -100])\n    print(f\"{decoded_string} \\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:31:47.321830Z","iopub.execute_input":"2023-06-20T04:31:47.322224Z","iopub.status.idle":"2023-06-20T04:31:47.816760Z","shell.execute_reply.started":"2023-06-20T04:31:47.322191Z","shell.execute_reply":"2023-06-20T04:31:47.815492Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"flattened_patches shape: torch.Size([2, 1024, 770]) \n\nattention_mask shape: torch.Size([2, 1024]) \n\nlabels shape: torch.Size([2, 153]) \n\n['<s_chart>', 'vertical', '_', 'bar', '</s_chart>', '<s_x_values>', '', '1', '9', '7', '6', ';', '1', '9', '7', '7', ';', '1', '9', '7', '8', ';', '1', '9', '7', '9', ';', '1', '9', '8', '0', ';', '1', '9', '8', '1', ';', '1', '9', '8', '2', ';', '1', '9', '8', '3', ';', '1', '9', '8', '4', ';', '1', '9', '8', '5', ';', '1', '9', '8', '6', ';', '1', '9', '8', '7', ';', '1', '9', '8', '8', ';', '1', '9', '8', '9', '</s_x_values>', '<s_y_values>', '', '1', '0', '4', '.', '6', ';', '8', '5', '.', '6', ';', '7', '9', '.', '4', ';', '7', '3', '.', '3', ';', '7', '4', '.', '5', ';', '8', '0', '.', '7', ';', '7', '9', '.', '4', ';', '9', '1', '.', '7', ';', '7', '2', '.', '7', ';', '6', '9', '.', '6', ';', '1', '0', '4', '.', '6', ';', '7', '0', '.', '8', ';', '8', '2', '.', '5', ';', '1', '0', '4', '.', '0', '</s_y_values>', '</s>'] \n\n['<s_chart>', 'line', '</s_chart>', '<s_x_values>', 'Van', 'u', 'atu', ';', 'V', 'ene', 'zu', 'ela', ';', 'Vi', 'et', 'nam', ';', 'Western', 'Africa', ';', 'Western', 'Asia', ';', 'Western', 'Europe', ';', 'Western', 'Sahara', ';', 'World', ';', 'Ye', 'men', ';', 'Za', 'm', 'bia', ';', 'Z', 'im', 'bab', 'we', '</s_x_values>', '<s_y_values>', '', '1', '0', '1', '.', '5', ';', '1', '0', '0', '.', '8', ';', '9', '8', '.', '2', ';', '9', '9', '.', '6', ';', '1', '0', '9', '.', '1', ';', '1', '0', '3', '.', '0', ';', '1', '2', '7', '.', '4', ';', '1', '0', '4', '.', '0', ';', '1', '0', '4', '.', '9', ';', '9', '2', '.', '4', ';', '9', '2', '.', '4', '</s_y_values>', '</s>'] \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### <b><span style='color:#F1A424'>Display Learning Rate</span></b>\n","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import OneCycleLR\n\nEPOCHS = config.EPOCHS\nBATCHES = len(train_dataloader)\nsteps = []\nlrs = []\noptim_lrs = []\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=1e-4,\n    epochs=config.EPOCHS,\n    steps_per_epoch=len(train_dataloader),\n    pct_start=0.1,\n    anneal_strategy=\"cos\",\n    final_div_factor=100,\n)\nfor epoch in range(EPOCHS):\n    for batch in range(BATCHES):\n        scheduler.step()\n        lrs.append(scheduler.get_last_lr()[0])\n        steps.append(epoch * BATCHES + batch)\n\nmax_lr = max(lrs)\nmin_lr = min(lrs)\nprint(f\"Maximum LR: {max_lr} | Minimum LR: {min_lr}\")\nplt.figure()\nplt.plot(steps, lrs, label='OneCycle')\nplt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\nplt.xlabel(\"Step\")\nplt.ylabel(\"Learning Rate\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-20T04:31:51.689989Z","iopub.execute_input":"2023-06-20T04:31:51.690419Z","iopub.status.idle":"2023-06-20T04:31:57.063491Z","shell.execute_reply.started":"2023-06-20T04:31:51.690380Z","shell.execute_reply":"2023-06-20T04:31:57.062480Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Maximum LR: 0.0001 | Minimum LR: 4e-08\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Train</b><a class='anchor' id='train'></a> [↑](#top) \n\n***","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import OneCycleLR\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=1e-4,\n    epochs=config.EPOCHS,\n    steps_per_epoch=len(train_dataloader),\n    pct_start=0.1,\n    anneal_strategy=\"cos\",\n    final_div_factor=100,\n)\n\ndevice = config.DEVICE\nepochs = config.EPOCHS\nmodel.to(device)\nmodel.train()\nlosses = []\n\nfor epoch in range(epochs):\n    print(f\"====== EPOCH {epoch} ======\")\n    loss_meter = AverageMeter()\n    with tqdm(train_dataloader, unit=\"train_batch\") as tqdm_train_dataloader:\n        for step, batch in enumerate(tqdm_train_dataloader):\n            flattened_patches = batch.pop(\"flattened_patches\").to(device)\n            attention_mask = batch.pop(\"attention_mask\").to(device)\n            labels = batch.pop(\"labels\").to(device)\n            outputs = model(\n                flattened_patches=flattened_patches,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            batch_size = flattened_patches.shape[0]\n            loss = outputs.loss\n            loss_meter.update(loss.item(), batch_size)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n    losses.append(loss_meter.avg)\n    print(f\"Average Loss: {loss_meter.avg}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Save Model</b><a class='anchor' id='save_model'></a> [↑](#top) \n\n***","metadata":{}},{"cell_type":"code","source":"processor.save_pretrained(\"/kaggle/working\")\nmodel.save_pretrained(\"/kaggle/working\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1A424'>|</span> Evaluate</b><a class='anchor' id='evaluate'></a> [↑](#top) \n\n***","metadata":{}},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    for step, batch in enumerate(tqdm(val_dataloader, unit=\"valid_batch\")):\n        flattened_patches = batch.pop(\"flattened_patches\").to(device)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n        prediction = model.generate(\n            flattened_patches=flattened_patches,\n            attention_mask=attention_mask,\n            max_new_tokens=512,\n            pad_token_id=processor.tokenizer.pad_token_id,\n            eos_token_id=processor.tokenizer.eos_token_id\n        )\n        if step%10==0:\n            print(f'Step: {step} | Prediction: {processor.batch_decode(prediction, skip_special_tokens=True)}'), print(\"\\n\")\n            print(f'Step: {step} | Ground Truth: {batch.pop(\"text\")}'), print(\"\\n\")\n            print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-05-19T20:10:15.813771Z","iopub.execute_input":"2023-05-19T20:10:15.814198Z","iopub.status.idle":"2023-05-19T20:10:51.969099Z","shell.execute_reply.started":"2023-05-19T20:10:15.814161Z","shell.execute_reply":"2023-05-19T20:10:51.968058Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":" 12%|█▎        | 1/8 [00:01<00:09,  1.32s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 0 | Prediction: ['<s_chart> vertical_bar</s_chart><s_x_values> Opera;Others;Edge;IE;Safari;Firefox;Chrome</s_x_values><s_y_values> 150;80;70;65;90;85;92;98</s_y_values>', '<s_chart> vertical_bar</s_chart><s_x_values> Eastern Thailand;Eastern Cambodia;SE Viet Nam;SW Viet Nam</s_x_values><s_y_values> 31.3;34.3;18.6;51.6</s_y_values>']\n\n\nStep: 0 | Ground Truth: ['<s_chart>vertical_bar</s_chart><s_x_values>Opera;Others;Edge;IE;Safari;Firefox;Chrome</s_x_values><s_y_values>102;154;169;246;404;404;2480</s_y_values>', '<s_chart>vertical_bar</s_chart><s_x_values>Eastern Thailand;Eastern Cambodia;SE viet Nam;SW Viet Nam</s_x_values><s_y_values>32.1;39.5;16.5;52.8</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 2/8 [00:02<00:08,  1.46s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 1 | Prediction: ['<s_chart> vertical_bar</s_chart><s_x_values> Germany;Switzerland;Hungary;Belgium;Indonesia;Niger;Vietnam;Saint Vincent and G.</s_x_values><s_y_values> 6.1;1.4;3.4;1.5;1.0;2.0;1.5;1.2;1.4;2.2</s_y_values>', '<s_chart> vertical_bar</s_chart><s_x_values> 11;12;13;14;15;16</s_x_values><s_y_values> 108.6;295.3;183.3;266.7;276.5;72.9</s_y_values>']\n\n\nStep: 1 | Ground Truth: ['<s_chart>vertical_bar</s_chart><s_x_values>Afghanistan;Libya Sanctions;Sudan (Darfur);Humanitarian issues in Syria;Children and Armed Conflict;Humanitarian issues in Syria;West Africa and the Sahel;Afghanistan;Non-proliferation of WMD;Guinea-Bissau;West Africa and the Sahel;Criminal Tribunals;Working Methods</s_x_values><s_y_values>4;0;0;0;3;0;0;2;0;2;0;1;1</s_y_values>', '<s_chart>vertical_bar</s_chart><s_x_values>11;12;13;14;15;16</s_x_values><s_y_values>107;297;184;259;262;71</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 3/8 [00:04<00:07,  1.47s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 2 | Prediction: ['<s_chart> line</s_chart><s_x_values> 1900;1995;2006;2010</s_x_values><s_y_values> 36.6;14.3;39.3;46.8;53.7;46.8</s_y_values>', '<s_chart> vertical_bar</s_chart><s_x_values> 2014;2015;2016;2017</s_x_values><s_y_values> 6.6;2.2;100.2;76.5</s_y_values>']\n\n\nStep: 2 | Ground Truth: ['<s_chart>line</s_chart><s_x_values>1990;2000;2010</s_x_values><s_y_values>45.3;42.8;39.2</s_y_values>', '<s_chart>vertical_bar</s_chart><s_x_values>2014;2015;2016;2017</s_x_values><s_y_values>5;25;100;75</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 4/8 [00:07<00:07,  1.94s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 3 | Prediction: ['<s_chart> line</s_chart><s_x_values> 0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18;19;20</s_x_values><s_y_values> 0.5042;1.2;3.0;2.9;3.4;2.6;1.1;3.7;4.9;5.4;4.7;3.0;2.6;1.4;1.0</s_y_values>', '<s_chart> vertical_bar</s_chart><s_x_values> Player 1;Player 2;Player 3;Player 4;Player 5</s_x_values><s_y_values> 10;40;19;16;20</s_y_values>']\n\n\nStep: 3 | Ground Truth: ['<s_chart>line</s_chart><s_x_values>0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18;19;20</s_x_values><s_y_values>0.0104;0.5221;1.0337;1.5453;1.9982;2.5;3.0;3.5;4.0;4.4;4.9;4.4;4.0;3.4;2.8;2.3;1.9921;1.4829;0.9587;0.4645;0.0152</s_y_values>', '<s_chart>vertical_bar</s_chart><s_x_values>Player 1;Player 2;Player 3;Player 4;Player 5</s_x_values><s_y_values>10;40;20;9;20</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▎   | 5/8 [00:28<00:27,  9.05s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 4 | Prediction: ['<s_chart> vertical_bar</s_chart><s_x_values> Salt tolerant;Salt tolerant (hypyl-alban;Salt tolerant (alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcoholic-alcohol', '<s_chart> vertical_bar</s_chart><s_x_values> Bottom Up;Left to Right;Up to Right;Top Down;Radial;Down to Right;3 (1%);2 (1%)</s_x_values><s_y_values> 92.2;62.2;43.2;6.1;5.2;3.1;2.1</s_y_values>']\n\n\nStep: 4 | Ground Truth: ['<s_chart>vertical_bar</s_chart><s_x_values>Salt tolerant T. Aman;Salt tolerant BRRI dhan;Salt tolerant Bina dhan;Salt tolerant alternative farming;Salt tolerant Aus;Vegetable farming;Resilient livestock rearing;Fish culture;Salt tolerant wheat;Salt tolerant potato;Salt tolerant pulse;Short duration oil seed;Salinity resistant jute;Salt tolerant sugarcane</s_x_values><s_y_values>40.2;18.4;17.9;10.3;52.1;65.2;76.6;88.0;4.8;2.1;4.8;5.9;2.1;18.4</s_y_values>', '<s_chart>vertical_bar</s_chart><s_x_values>Bottom Up;Left to Right;Up to Right;Top Down;Radial;Down to Right;Right to Left;Circular</s_x_values><s_y_values>90;62;43;7;7;6;4;2</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 6/8 [00:31<00:13,  6.83s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 5 | Prediction: ['<s_chart> line</s_chart><s_x_values> 40;80;120;160;200;240;280;320;360;400</s_x_values><s_y_values> 0.3161;1.2034;2.6;3.4;5.2;6.2;7.2;8.4</s_y_values>', '<s_chart> line</s_chart><s_x_values> 1800;1840;1860;1880;1900;1920;1940;1960;1980;2000</s_x_values><s_y_values> 1047;1055;1080;1065;1097;1104;1205;1374;597;5720;5475;5910;9472</s_y_values>']\n\n\nStep: 5 | Ground Truth: ['<s_chart>line</s_chart><s_x_values>40;80;120;160;200;240;280;320;360;400</s_x_values><s_y_values>0.4703;1.2713;2.1;3.1;4.0;4.8;5.7;6.6;7.4;8.3</s_y_values>', '<s_chart>line</s_chart><s_x_values>1820;1840;1860;1880;1900;1920;1940;1960;1980;2000</s_x_values><s_y_values>5953;10361;14663;19558;25899;21934;55271;50550;90911;86780</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 7/8 [00:32<00:04,  4.94s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 6 | Prediction: ['<s_chart> line</s_chart><s_x_values> 1;2;3;4;5;6;7;10</s_x_values><s_y_values> 9.5;23.3;25.2;47.2;59.7;75.4</s_y_values>', '<s_chart> line</s_chart><s_x_values> 10;20;30;40;50;60</s_x_values><s_y_values> 1;2;4;8;1;5;4;9</s_y_values>']\n\n\nStep: 6 | Ground Truth: ['<s_chart>line</s_chart><s_x_values>1;2;3;4;5;6;7;8;9;10</s_x_values><s_y_values>10;20;30;25;21;40;50;35;16;71</s_y_values>', '<s_chart>line</s_chart><s_x_values>10;20;30;40</s_x_values><s_y_values>4;8;8;8</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8/8 [00:36<00:00,  4.52s/valid_batch]","output_type":"stream"},{"name":"stdout","text":"Step: 7 | Prediction: ['<s_chart> line</s_chart><s_x_values> 1925;1930;1935;1940;1945;1960;1965;1970;1975;1978;1980;1985;1996;2000;2004;2010</s_x_values><s_y_values> 1325;1080;1935;1940;1947;1950;1965;1960;1970;1975;1980;1985;1996;2000;2006</s_y_values>', '<s_chart> horizontal_bar</s_chart><s_x_values> 0.015;10.0;20.0;30.0;40.0;50.0;60.0;70.0;80.0;90.0;100.0;15.0</s_y_values>']\n\n\nStep: 7 | Ground Truth: ['<s_chart>line</s_chart><s_x_values>1925;1930;1935;1940;1947;1950;1955;1960;1965;1970;1975;1980;1985;1990;1995;2000;2005;2010</s_x_values><s_y_values>135834;135834;148182;147342;108735;189344;189344;189344;268822;555611;1467474;2691805;4264349;6120660;7890864;10809896;14719415;17022788</s_y_values>', '<s_chart>horizontal_bar</s_chart><s_x_values>38.6;1.1029;30.5;10.2;1.3235;4.7;1.3235;3.2;10.2</s_x_values><s_y_values>Opportunity to save and receive loans;Protection against harassment from offenders or others;Better information about alternative job offers;Access to health and pension;More safety at the workplace;Enhanced information that healps me to earn more in the job;More stable income;Higher income;A more secure job contract</s_y_values>']\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}