{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUMrT5nTlSb4"
   },
   "source": [
    "# COLAB TPU 2.8 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoE4ss-00sO3"
   },
   "outputs": [],
   "source": [
    "################## EFF_NETS = [ ？ ]*FOLDS #######################\n",
    "#########挂载硬盘上训练，每折都保存最优模型，N_TRAINED_FOLD>0为接着训练##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZvHbuWUu_m8"
   },
   "outputs": [],
   "source": [
    "######## 已经训练完的折数########\n",
    "N_TRAINED_FOLD = 0\n",
    "USE_TRAINED_MODEL = True # False = 测试N_TRAINED_FOLD后的步骤\n",
    "\n",
    "# 快速测试跑通一遍\n",
    "FAST_KAGGING = False\n",
    "\n",
    "################可视化################\n",
    "# VERBOSE = 0\n",
    "# DISPLAY_PLOT = False # 显示训练图\n",
    "VERBOSE = 1\n",
    "DISPLAY_PLOT = True # 显示训练图\n",
    "DISPLAY_TRAINED_PLOT = True # 显示已经训练的model图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "rH3uNiCKo1wn",
    "outputId": "a017ce4a-734b-4279-e8b0-118a52bfe153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Colab google\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# 运行目录\n",
    "import os\n",
    "os.chdir(\"/content/drive/My Drive/siim\") ######### My Drive ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX1a3X6C5BSi"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yB3waQXOf1_E",
    "outputId": "00fe94eb-98b3-4241-a723-d99e2493f3f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow~=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 16kB/s \n",
      "\u001b[?25hCollecting tensorflow_gcs_config~=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/0b/c6fe9b75ba3999c10b7ab77e7d4019c90344bb79609a9378355ca6712598/tensorflow_gcs_config-2.2.0-py3-none-any.whl (392kB)\n",
      "\u001b[K     |████████████████████████████████| 399kB 35.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.30.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.9.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.12.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.34.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 61.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (1.18.5)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.2.0) (0.3.3)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 54.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow~=2.2.0) (49.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.17.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow~=2.2.0) (0.4.8)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, tensorflow-gcs-config\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "  Found existing installation: tensorflow-gcs-config 2.3.0\n",
      "    Uninstalling tensorflow-gcs-config-2.3.0:\n",
      "      Successfully uninstalled tensorflow-gcs-config-2.3.0\n",
      "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0 tensorflow-gcs-config-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import os\n",
    "resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n",
    "if resp.status_code != 200:\n",
    "  print(\"Failed to switch the TPU to TF {}\".format(version))\n",
    "\n",
    "####################################\n",
    "# colab pro这段时间内存报错的问题，加一段代码就可以了 from JWD\n",
    "# https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/172357，\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dUH4nJn2g-Ts",
    "outputId": "82e05a3c-da4c-4b94-8f0a-ed4f9e90b965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018, 2019, 2020]\n"
     ]
    }
   ],
   "source": [
    "############### data if kagging ###################\n",
    "KAGGING = False # \n",
    "SEED_TEST = list(range(2021)) # KAGGING_TEST RANGE\n",
    "KAGGING_SEED = 24\n",
    "print(SEED_TEST[-3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eRidcAReEdME",
    "outputId": "9d9f08fc-cf45-4904-e96b-5691b0e7a7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s eta 0:00:011\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWyz95iHkSlh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf, re, math\n",
    "import tensorflow.keras.backend as K\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "1_jz1JvAk--g",
    "outputId": "8c1bca25-e96b-4860-cbe3-d8c6ab0fde59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team4 seed:550550\n",
      "SIIM_parameters: {'FOLDS': 5, 'IMG_SIZES': [384, 384, 384, 384, 384], 'TTA': 13, 'ls': 0.015, 'INC2019': [1, 1, 1, 1, 1], 'INC2018': [1, 1, 1, 1, 1], 'BATCH_SIZES': [8, 8, 8, 8, 8], 'EPOCHS': [20, 20, 20, 20, 20], 'EFF_NETS': [6, 6, 6, 6, 6], 'MALIG': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"TPU\" # \"GPU\" \n",
    "\n",
    "\n",
    "# SEED = 42\n",
    "if KAGGING: SEEDS = [42]\n",
    "else: SEEDS = [1911, 2822, 3733, 4644, 55055, 6466, 7377, 8288, 9199]\n",
    "\n",
    "\n",
    "# FOLDS = 5\n",
    "FOLDS = 2 if FAST_KAGGING else 5 # 2\n",
    "\n",
    "#  384, 512, 768 image size\n",
    "# IMG_SIZES = [ 512 ] * FOLDS\n",
    "IMG_SIZES = [ 384 ]*FOLDS if FAST_KAGGING else [ 384 ]*FOLDS\n",
    "\n",
    "# bacth size\n",
    "BATCH_SIZES = [ 32 ]*FOLDS if FAST_KAGGING else [ 8 ]*FOLDS # 32太大了，16 >> B6-512-15bs\n",
    "\n",
    "\n",
    "EPOCHS = [ 2 ]*FOLDS if FAST_KAGGING else [ 20 ]*FOLDS\n",
    "\n",
    "\n",
    "\n",
    "# EFF_NETS = [ 6 ]*FOLDS\n",
    "EFF_NETS = [ 6 ]*FOLDS if FAST_KAGGING else [ 6 ]*FOLDS\n",
    "\n",
    "\n",
    "\n",
    "# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\n",
    "WGTS = [1/FOLDS]*FOLDS\n",
    "# WGTS = [.25,.25,0,.25,.25] ##temp## ########调整各个fold的pred的权重（根据Local CV）###########\n",
    "\n",
    "\n",
    "############差异化融合###################\n",
    "# (2018/2017) 和 2019数据集\n",
    "INC2019 = [  1 ] * FOLDS\n",
    "INC2018 = [  1 ] * FOLDS\n",
    "# INC2019 = [  0 ] * FOLDS\n",
    "# INC2018 = [  0 ] * FOLDS\n",
    "\n",
    "############差异化融合###################\n",
    "# 由于本次只有2%的得病，进行上采样\n",
    "# 上采样malig\n",
    "# 如果2018和2019外源数据都用了，只会加500张没见过的\n",
    "# 如果只用2018，则加2019和500张没见过的\n",
    "# 如果只用2019，则加2018和500张没见过的\n",
    "# 如果都没用，则加2018，2019和500张没见过的\n",
    "MALIG = [ 1 ]*FOLDS\n",
    "# MALIG = [ 0 ]*FOLDS\n",
    "\n",
    "\n",
    "# TEST TIME AUGMENTATION STEPS，10+ steps\n",
    "# TTA = 10\n",
    "TTA = 13\n",
    "# 标签平滑，可以尝试0.015，抗过拟合\n",
    "# ls = 0.02\n",
    "ls = 0.015\n",
    "\n",
    "\n",
    "# 训练schedule\n",
    "def get_lr_callback(batch_size=8):\n",
    "    #############需细致调整###################\n",
    "    # lr_start   = 1e-6 # 初始学习率\n",
    "    # lr_max   =  2e-4# 最大学习率\n",
    "    # lr_min     = 1e-7 #最小学习率\n",
    "    # lr_ramp_ep =  10 # 用几个epoch达到最大学习率\n",
    "    # lr_sus_ep  =  2# 用最大的学习率跑几个epoch\n",
    "    # lr_decay   = .5 # 退火\n",
    "    ################## 8月 #####################\n",
    "    # lr_start   = 5e-6 # 初始学习率\n",
    "    # lr_max   =  1e-4# 最大学习率\n",
    "    # lr_min     = 1e-6 #最小学习率\n",
    "    # lr_ramp_ep =  14 # 用几个epoch达到最大学习率\n",
    "    # lr_sus_ep  =  1 # 用最大的学习率跑几个epoch\n",
    "    # lr_decay   = .4 # 退火\n",
    "    lr_start   = 1e-5 # 初始学习率\n",
    "    lr_max   =  1e-4# 最大学习率\n",
    "    lr_min     = 1e-6 #最小学习率\n",
    "    lr_ramp_ep =  20 # 用几个epoch达到最大学习率\n",
    "    lr_sus_ep  =  1 # 用最大的学习率跑几个epoch\n",
    "    lr_decay   = .4 # 退火\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback\n",
    "\n",
    "# 每一个epoch的情况\n",
    "# VERBOSE = 1\n",
    "# DISPLAY_PLOT = True\n",
    "\n",
    "SEED = KAGGING_SEED\n",
    "\n",
    "\n",
    "print('seed:{}'.format(SEED))\n",
    "\n",
    "SIIM_para = {}\n",
    "\n",
    "# SIIM_para['SEED'] = SEED\n",
    "SIIM_para['FOLDS'] = FOLDS\n",
    "SIIM_para['IMG_SIZES'] = IMG_SIZES\n",
    "SIIM_para['TTA'] = TTA\n",
    "SIIM_para['ls'] = ls\n",
    "SIIM_para['INC2019'] = INC2019\n",
    "SIIM_para['INC2018'] = INC2018\n",
    "SIIM_para['BATCH_SIZES'] = BATCH_SIZES\n",
    "SIIM_para['EPOCHS'] = EPOCHS\n",
    "SIIM_para['EFF_NETS'] = EFF_NETS\n",
    "SIIM_para['MALIG'] = MALIG\n",
    "\n",
    "\n",
    "print('SIIM_parameters: {}'.format(SIIM_para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbcmOWWVynD3"
   },
   "outputs": [],
   "source": [
    "assert (IMG_SIZES==[ 384 ]*FOLDS) & (EFF_NETS == [ 6 ]*FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3v4KVCFT6Pp"
   },
   "outputs": [],
   "source": [
    "if not KAGGING:\n",
    "    assert SEED not in SEED_TEST+[42]\n",
    "else:\n",
    "    assert SEED not in SEEDS+[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "N6zrcZOSsf2S",
    "outputId": "c368d325-c778-4a2d-ade1-9adfe9646647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Running on TPU  grpc://10.11.163.106:8470\n",
      "initializing  TPU ...\n",
      "INFO:tensorflow:Initializing the TPU system: grpc://10.11.163.106:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.11.163.106:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU initialized\n",
      "REPLICAS: 8\n"
     ]
    }
   ],
   "source": [
    "# 使用tpu\n",
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-f-5po_aslYH"
   },
   "outputs": [],
   "source": [
    "# #### UnimplementedError TPU不支持local load，只能通过google storage ######\n",
    "# GCS_PATH = ['./input/melanoma-{}x{}'.format(IMG_SIZES[0],IMG_SIZES[0])]*FOLDS \n",
    "# GCS_PATH2 = ['./input/isic2019-{}x{}'.format(IMG_SIZES[0],IMG_SIZES[0])]*FOLDS\n",
    "# GCS_PATH3 = ['./input/malignant-v2-{}x{}'.format(IMG_SIZES[0],IMG_SIZES[0])]*FOLDS\n",
    "\n",
    "if IMG_SIZES[0] == 256:\n",
    "  GCS_PATH = ['gs://kds-f8af4bf2f6c765dbd327683a170e364332a1750658e910153e98d0d5']*FOLDS\n",
    "  GCS_PATH2 = ['gs://kds-436e76b7e255e4499d5d11da9d7ab87646e271e08ec1d546f310ec00']*FOLDS\n",
    "  GCS_PATH3 = ['gs://kds-9e5912113bdca38bda75e6858a127bdfd0f26951b4f6843c45974106']*FOLDS\n",
    "elif IMG_SIZES[0] == 384: \n",
    "  GCS_PATH = ['gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4']*FOLDS\n",
    "  GCS_PATH2 = ['gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee']*FOLDS\n",
    "  GCS_PATH3 = ['gs://kds-c557c41c0102dc29c1ed0386c4746604d132a95f29eeacf42e522004']*FOLDS\n",
    "elif IMG_SIZES[0] == 512:\n",
    "  GCS_PATH = ['gs://kds-458e4ec4a30e5d927b21efd0633fb2c0a091245fe8db3c415fde5da6']*FOLDS\n",
    "  GCS_PATH2 = ['gs://kds-203bea6cc14c2866536ac303b09a6e77a6b3d0a371e96b680836433c']*FOLDS\n",
    "  GCS_PATH3 = ['gs://kds-00457c0c555bfdd50b1d2db15abd7feb19d4795037f919a3dc0abbee']*FOLDS\n",
    "elif IMG_SIZES[0] == 768:\n",
    "  GCS_PATH = ['gs://kds-cfb16ed5f55adb5ab35a75a2fe74c3dc11a4b869dc8cfb3d9b1759e1']*FOLDS\n",
    "  GCS_PATH2 = ['gs://kds-79d9c56dcae7978f98a6bbe8318ac6866ba4778fc95c5871fcfd0f03']*FOLDS\n",
    "  GCS_PATH3 = ['gs://kds-032edf427ad6e338f5392f73759ff404a1a3f25c2abdc4995136de30']*FOLDS\n",
    "else:\n",
    "  print('Please choose a value between 384, 512 or 768')\n",
    "\n",
    "files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/train*.tfrec')))\n",
    "files_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPeJGLyYromm"
   },
   "outputs": [],
   "source": [
    "ROT_ = 180.0\n",
    "SHR_ = 2.0\n",
    "HZOOM_ = 8.0\n",
    "WZOOM_ = 8.0\n",
    "HSHIFT_ = 8.0\n",
    "WSHIFT_ = 8.0\n",
    "\n",
    "\n",
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "\n",
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "# transform augmentation by chris\n",
    "def transform(image, DIM=256):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = ROT_ * tf.random.normal([1], dtype='float32')\n",
    "    shr = SHR_ * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n",
    "    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])\n",
    "\n",
    "# 读取tfrecord\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['target']\n",
    "\n",
    "# 读取test set的tfrecord\n",
    "def read_unlabeled_tfrecord(example, return_image_name):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['image_name'] if return_image_name else 0\n",
    "\n",
    " # 图片\n",
    "def prepare_image(img, augment=True, dim=256):    \n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    if augment:\n",
    "        img = transform(img,DIM=dim)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        # img = tf.image.random_hue(img, 0.01)\n",
    "        # img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        # img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        # img = tf.image.random_brightness(img, 0.1)\n",
    "        # 没啥用\n",
    "                      \n",
    "    img = tf.reshape(img, [dim,dim, 3])\n",
    "            \n",
    "    return img\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "\n",
    "def get_dataset(files, augment = False, shuffle = False, repeat = False, \n",
    "                labeled=True, return_image_names=True, batch_size=16, dim=256):\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "        \n",
    "    if labeled: \n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), num_parallel_calls=AUTO)      \n",
    "    # ###########################数据增强############################\n",
    "    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, dim=dim), imgname_or_label), num_parallel_calls=AUTO)\n",
    "    \n",
    "    ds = ds.batch(batch_size * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "def build_model(dim=128, ef=0):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    base = EFNS[ef](input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n",
    "    x = base(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inp,outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=ls) \n",
    "    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "d6wwsW1dEDfQ",
    "outputId": "b8f36adb-499a-486b-e925-58166682de7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range 0 (array([ 0,  2,  3,  5,  6,  8,  9, 10, 11, 12, 13, 14]), array([1, 4, 7]))\n",
      "WARNING:tensorflow:TPU system grpc://10.11.163.106:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.11.163.106:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.11.163.106:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.11.163.106:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "#### FOLD 1\n",
      "#### Image Size 384 with EfficientNet B6 and batch_size 64\n",
      "#### Using 2019 external data\n",
      "['gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train00-2182.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train02-2193.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train03-2182.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train05-2171.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train06-2175.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train08-2177.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train09-2178.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train10-2174.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train11-2176.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train12-2198.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train13-2186.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train14-2174.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train01-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train05-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train07-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train11-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train13-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train17-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train19-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train21-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train23-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train25-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train27-828.tfrec', 'gs://kds-6f9d2143e7fe97eccac0219c1aacb107a234b440670f250683ba40ee/train29-821.tfrec']\n",
      "#### Using 2018+2017 external data\n",
      "#### Using unseen extra malignant data!!!!!!!!!!\n",
      "#########################\n",
      "['gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train01-2185.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train04-2167.tfrec', 'gs://kds-71b679af6661316328cfa122f5342b4a84fe0c7c1135d809fbee8ca4/train07-2174.tfrec']\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "165527552/165527152 [==============================] - 2s 0us/step\n",
      "Training...\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733/733 [==============================] - 292s 399ms/step - auc: 0.7409 - loss: 0.3562 - val_auc: 0.7762 - val_loss: 0.1256 - lr: 1.0000e-05\n",
      "Epoch 2/20\n",
      "445/733 [=================>............] - ETA: 1:39 - auc: 0.8243 - loss: 0.2996"
     ]
    }
   ],
   "source": [
    "oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \n",
    "preds = np.zeros((count_data_items(files_test),1))\n",
    "skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n",
    "# Train_val_split\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):   \n",
    "    print('Range', fold, (idxT,idxV))\n",
    "    #############未训练的###################\n",
    "    # if fold >= N_TRAINED_FOLD:\n",
    "    # DISPLAY FOLD INFO\n",
    "    if DEVICE=='TPU':\n",
    "        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    print('#'*25); print('#### FOLD',fold+1)\n",
    "    print('#### Image Size %i with EfficientNet B%i and batch_size %i'%\n",
    "        (IMG_SIZES[fold],EFF_NETS[fold],BATCH_SIZES[fold]*REPLICAS))\n",
    "    \n",
    "    # CREATE TRAIN AND VALIDATION SUBSETS\n",
    "    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxT])\n",
    "    if INC2019[fold]:\n",
    "        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2+1])\n",
    "        print('#### Using 2019 external data')\n",
    "        print(files_train)\n",
    "    if INC2018[fold]:\n",
    "        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '/train%.2i*.tfrec'%x for x in idxT*2])\n",
    "        print('#### Using 2018+2017 external data')\n",
    "        # print(files_train)\n",
    " \n",
    "    # malignant upsample\n",
    "    if MALIG[fold]:\n",
    "        if INC2018[fold] and INC2019[fold]:\n",
    "            malig_idx = [i for i in range(15,30)] # 15 - 30 unseen\n",
    "            files_train += tf.io.gfile.glob([GCS_PATH3[fold] + '/train%.2i*.tfrec'%x for x in malig_idx])\n",
    "            print('#### Using unseen extra malignant data!!!!!!!!!!')\n",
    "            # print(files_train)\n",
    "        elif INC2018[fold]:\n",
    "            malig_idx = [i for i in range(15,30)] + [j for j in range(31, 60,2)]  # 15 - 30 unseen + 31 - 59 odd\n",
    "            files_train += tf.io.gfile.glob([GCS_PATH3[fold] + '/train%.2i*.tfrec'%x for x in malig_idx])\n",
    "            print('#### Using 2019 + unseen extra malignant data!!!!!!!!!!')\n",
    "            # print(files_train)\n",
    "        elif INC2019[fold]:\n",
    "            malig_idx = [i for i in range(15,30)] + [j for j in range(30, 60,2)]  # 15 - 30 unseen + 30 - 58 even\n",
    "            files_train += tf.io.gfile.glob([GCS_PATH3[fold] + '/train%.2i*.tfrec'%x for x in malig_idx])\n",
    "            print('#### Using 2018 + unseen extra malignant data!!!!!!!!!!')\n",
    "            # print(files_train)\n",
    "        else:\n",
    "            malig_idx = [i for i in range(15,60)] # 15 - 59 this includes all 2018 and 2019\n",
    "            files_train += tf.io.gfile.glob([GCS_PATH3[fold] + '/train%.2i*.tfrec'%x for x in malig_idx])\n",
    "            print('#### Using all extra malignant data!!!!!!!!!!')\n",
    "\n",
    "\n",
    "    np.random.shuffle(files_train); print('#'*25)\n",
    "    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxV])\n",
    "    print(files_valid)\n",
    "    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '/test*.tfrec')))\n",
    "    \n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n",
    "        \n",
    "    # SAVE BEST MODEL EACH FOLD\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%(fold+1), monitor='val_auc', verbose=0, save_best_only=True, save_weights_only=True, mode='max', save_freq='epoch')\n",
    "\n",
    "    ##############未训练的###################\n",
    "    if fold >= N_TRAINED_FOLD:\n",
    "        # TRAIN\n",
    "        print('Training...')\n",
    "        history = model.fit(\n",
    "            get_dataset(files_train, augment=True, shuffle=True, repeat=True, dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n",
    "            epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \n",
    "            steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\n",
    "            validation_data=get_dataset(files_valid,augment=False,shuffle=False, repeat=False,dim=IMG_SIZES[fold]), #class_weight = {0:1,1:2},\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "\n",
    "        # save history pickle\n",
    "        print('Saving train history...') \n",
    "        import pickle\n",
    "        with open('hist-fold-%i.pickle'%(fold+1), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "\n",
    "        # print('Loading best model fold-%i.h5...'%(fold+1))\n",
    "        # model.load_weights('fold-%i.h5'%(fold+1))\n",
    "\n",
    "        print('Loading best model fold-%i.h5...'%(fold+1))\n",
    "        model.load_weights('fold-%i.h5'%(fold+1))\n",
    "        # PREDICT OOF USING TTA\n",
    "        print('Predicting OOF with TTA...')\n",
    "        ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True, repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "        ct_valid = count_data_items(files_valid)\n",
    "        STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n",
    "        pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n",
    "        oof_pred.append(np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n",
    "        #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n",
    "        \n",
    "        # GET OOF TARGETS AND NAMES\n",
    "        ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=True, return_image_names=True)\n",
    "        oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n",
    "        oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n",
    "        ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=False, return_image_names=True)\n",
    "        oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n",
    "        \n",
    "        # PREDICT TEST USING TTA\n",
    "        print('Predicting Test with TTA...')\n",
    "        ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n",
    "                repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "        ct_test = count_data_items(files_test)\n",
    "        STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n",
    "        pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n",
    "\n",
    "\n",
    "        #########Data if Kagging###########\n",
    "        # save temp pred\n",
    "        pred_temp = np.zeros((count_data_items(files_test),1))\n",
    "        pred_temp[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1)\n",
    "\n",
    "        ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=False, return_image_names=True)\n",
    "        image_names = np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())])\n",
    "\n",
    "        submission_temp = pd.DataFrame(dict(image_name=image_names, target=pred_temp[:,0]))\n",
    "        submission_temp = submission_temp.sort_values('image_name') \n",
    "        submission_temp.to_csv('submission_temp_fold{}.csv'.format(fold+1), index=False)\n",
    "        # pred_temp.to_csv('pred_temp_fold{}'.format(fold+1))\n",
    "        #######################\n",
    "\n",
    "\n",
    "        preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]    \n",
    "        # REPORT RESULTS\n",
    "        auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n",
    "\n",
    "        oof_val.append(np.max( history.history['val_auc'] ))\n",
    "        print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n",
    "        # 2\n",
    "        if DISPLAY_PLOT:\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
    "            plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n",
    "            x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n",
    "            xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "            plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
    "            plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n",
    "            plt.legend(loc=2)\n",
    "            plt2 = plt.gca().twinx()\n",
    "            plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "            plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
    "            x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
    "            ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "            plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "            plt.ylabel('Loss',size=14)\n",
    "            plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n",
    "                    (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n",
    "            plt.legend(loc=3)\n",
    "            plt.show()  \n",
    "\n",
    "    else:\n",
    "        ############## 已经训练的 ###################\n",
    "        print('>>> Fold-%i was trained...'%(fold+1))\n",
    "        if USE_TRAINED_MODEL:\n",
    "            \n",
    "            print('Use trained model >>>')\n",
    "            # print('Loading best model fold-%i.h5...'%(fold+1))\n",
    "            # model.load_weights('fold-%i.h5'%(fold+1))\n",
    "\n",
    "            # ##############\n",
    "            # # BUILD MODEL\n",
    "            # K.clear_session()\n",
    "            # with strategy.scope():\n",
    "            #     model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n",
    "            # ##############\n",
    "\n",
    "            # load history pickle\n",
    "            print('Loading train history...') \n",
    "            import pickle\n",
    "            with open('hist-fold-%i.pickle'%(fold+1), 'rb') as f:\n",
    "                history = pickle.load(f)        \n",
    "\n",
    "\n",
    "            print('Loading best model fold-%i.h5...'%(fold+1))\n",
    "            model.load_weights('fold-%i.h5'%(fold+1))\n",
    "            # PREDICT OOF USING TTA\n",
    "            print('Predicting OOF with TTA...')\n",
    "            ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True, repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "            ct_valid = count_data_items(files_valid)\n",
    "            STEPS = TTA * ct_valid/BATCH_SIZES[fold]/4/REPLICAS\n",
    "            pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n",
    "            oof_pred.append(np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n",
    "            #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n",
    "            \n",
    "            # GET OOF TARGETS AND NAMES\n",
    "            ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=True, return_image_names=True)\n",
    "            oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n",
    "            oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n",
    "            ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=False, return_image_names=True)\n",
    "            oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n",
    "            \n",
    "            # PREDICT TEST USING TTA\n",
    "            print('Predicting Test with TTA...')\n",
    "            ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n",
    "                    repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n",
    "            ct_test = count_data_items(files_test)\n",
    "            STEPS = TTA * ct_test/BATCH_SIZES[fold]/4/REPLICAS\n",
    "            pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n",
    "\n",
    "\n",
    "            #########Data if Kagging###########\n",
    "            # save temp pred\n",
    "            pred_temp = np.zeros((count_data_items(files_test),1))\n",
    "            pred_temp[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1)\n",
    "\n",
    "            ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=False, return_image_names=True)\n",
    "            image_names = np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())])\n",
    "\n",
    "            submission_temp = pd.DataFrame(dict(image_name=image_names, target=pred_temp[:,0]))\n",
    "            submission_temp = submission_temp.sort_values('image_name') \n",
    "            submission_temp.to_csv('submission_temp_fold{}.csv'.format(fold+1), index=False)\n",
    "            # pred_temp.to_csv('pred_temp_fold{}'.format(fold+1))\n",
    "            #######################\n",
    "\n",
    "\n",
    "            preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]    \n",
    "            # REPORT RESULTS\n",
    "            auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n",
    "\n",
    "            oof_val.append(np.max( history['val_auc'] ))\n",
    "            print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n",
    "\n",
    "            # PLOT TRAINING\n",
    "            # if DISPLAY_PLOT:\n",
    "            if DISPLAY_PLOT and DISPLAY_TRAINED_PLOT:\n",
    "                plt.figure(figsize=(15,5))\n",
    "                plt.plot(np.arange(EPOCHS[fold]),history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
    "                plt.plot(np.arange(EPOCHS[fold]),history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n",
    "                x = np.argmax( history['val_auc'] ); y = np.max( history['val_auc'] )\n",
    "                xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "                plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
    "                plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n",
    "                plt.legend(loc=2)\n",
    "                plt2 = plt.gca().twinx()\n",
    "                plt2.plot(np.arange(EPOCHS[fold]),history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
    "                plt2.plot(np.arange(EPOCHS[fold]),history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
    "                x = np.argmin( history['val_loss'] ); y = np.min( history['val_loss'] )\n",
    "                ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "                plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
    "                plt.ylabel('Loss',size=14)\n",
    "                plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n",
    "                        (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n",
    "                plt.legend(loc=3)\n",
    "                plt.show()  \n",
    "    ######################################### \n",
    "\n",
    "# COMPUTE OVERALL OOF AUC\n",
    "oof = np.concatenate(oof_pred)\n",
    "true = np.concatenate(oof_tar)\n",
    "names = np.concatenate(oof_names)\n",
    "folds = np.concatenate(oof_folds)\n",
    "auc = roc_auc_score(true, oof)\n",
    "print('Overall OOF AUC with TTA = %.3f'%auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erw1YNQxKkps"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9d0kNGyq1cf"
   },
   "outputs": [],
   "source": [
    "# SAVE OOF TO DISK\n",
    "df_oof = pd.DataFrame(dict(image_name=names, target=true, pred=oof, fold=folds))\n",
    "df_oof.to_csv('oof.csv',index=False)\n",
    "df_oof.head()\n",
    "\n",
    "ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold], labeled=False, return_image_names=True)\n",
    "image_names = np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())])\n",
    "\n",
    "submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\n",
    "submission = submission.sort_values('image_name') \n",
    "submission.to_csv('submission.csv', index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTsic3ir6FKM"
   },
   "outputs": [],
   "source": [
    "# submission.to_csv('submission.csv', index=False)\n",
    "sub_file_name = 'submissionFAST_RUN-{}.csv'.format(SEED)\n",
    "\n",
    "submission.to_csv(sub_file_name, index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXV3dNFweGg6"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "# files.download('submission.csv')\n",
    "# files.download(sub_file_name)\n",
    "# files.download('oof.csv') \n",
    "print(sub_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av0QSG69C47u"
   },
   "outputs": [],
   "source": [
    "# submission.to_csv('submission.csv', index=False)\n",
    "files.download('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKTUc3pBouLo"
   },
   "outputs": [],
   "source": [
    "# files.download('oof.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPxgDmaVpwgM"
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "use_time = end - start\n",
    "print(end)\n",
    "print('The total time is {} minutes'.format(round(use_time/60,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joHa-3MMExZD"
   },
   "outputs": [],
   "source": [
    "print(time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvQBDoW25WkA"
   },
   "outputs": [],
   "source": [
    "#SIIM_para['run_time'] = round(use_time/60,1)\n",
    "print('SIIM_parameters: {}'.format(SIIM_para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-Z9-f8pfTVs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "siim_LocalRUN-done_subRUN8-9-1-384B6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
